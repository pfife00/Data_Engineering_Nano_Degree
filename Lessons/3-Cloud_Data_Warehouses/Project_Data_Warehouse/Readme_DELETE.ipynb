{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Project: Data Warehouse</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Installation</h3>\n",
    "\n",
    "The project requires <b> Python 3.x </b> and <a href=\"http://initd.org/psycopg/docs/\" rel=\"nofollow\">Psycopg</a>. \n",
    "Pyscopg can be installed on your computer by running the following command in the terminal window:\n",
    "<br><code>$ pip install psycopg2</code></br>\n",
    "\n",
    "Also the following module is required from the Python Standard Library:\n",
    "<ul>\n",
    "\n",
    "  <li> <a href=\"https://docs.python.org/3/library/configparser.html\" rel=\"nofollow\">configparser</a> </li>\n",
    "\n",
    "</ul>\n",
    "If you wish to use Infrastructure as Code (IAC) then the following Python library should be intalled:\n",
    "<ul>\n",
    "\n",
    "  <li> <a href=\"http://pandas.pydata.org\" rel=\"nofollow\">Pandas</a> </li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Project Motivation</h3>\n",
    "The project's goal is to build an ETL pipeline for a database hosted on AWS redshift. The music streaming startup, Sparkify, has grown their user base and song base and wants to move their processes and data into the cloud. The data currently resides in AWS S3 which contains a directory of JSON logs and a directory of JSON metadata. The objective is to build an ETL pipeline which extracts the data from S3, stages the data in Redshift, and transforms the data into a set of dimensional tables in order for the analytics team to continue finding insights to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Third_Point_Header\">File Descriptions</h3>\n",
    "The files required to run the function are organized as follows:\n",
    "<ul>\n",
    "    <li> create_tables.py - Drops and creates tables from sql_queries.py file. Also loads the parameters stored within the dwh.cfg file. Note, create_tables.py should be run prior to running etl script.</li>\n",
    "    <li> etl.py - Copies the data from S3 to the staging tables. Inserts the data to the appropriate defined tables.</li>\n",
    "    <li> sql_queries.py - Contains all the sql queries, which are imported into the etl pipeline and create_tables.py files.</li>\n",
    "    <li> dwh.cfg - Contains the necessary credentials to connect to redshift database.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Instructions to Load the JSON files </h3>\n",
    "The data are contained in two datasets consisting of JSON files. The Song dataset is stored in S3 and can accessed from the link below.\n",
    "<br>s3://udacity-dend/song_data</br>\n",
    "<br>The Log dataset is stored in S3 and can be accessed from the link below.</br>\n",
    "s3://udacity-dend/log_json_path.json\n",
    "The datasets are copied from S3 to the two staging tables defined within sql_queries.py.\n",
    "<ul>\n",
    "    <li> staging_events_table - stores the data from the Log dataset</li>\n",
    "    <li> staging_songs_table - stores the data from the song dataset.</li>\n",
    "</ul>\n",
    "The data are copied from S3 to the staging_events_table by executing the below code within sql_queries.py\n",
    "<pre>\n",
    "<codeblock>\n",
    "staging_events_copy = (\"\"\"\n",
    "    COPY staging_events_table from 's3://udacity-dend/log_data'\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE off\n",
    "    REGION 'us-west-2'\n",
    "    JSON 's3://udacity-dend/log_json_path.json';\n",
    "    \"\"\").format(config.get('IAM_ROLE', 'ARN') )\n",
    "    </codeblock>\n",
    "</pre>\n",
    "\n",
    "The data are copied from S3 to the staging_songs_table by executing the below code within sql_queries.py\n",
    "<pre>\n",
    "<codeblock>\n",
    "staging_songs_copy = (\"\"\"\n",
    "    COPY staging_songs_table from 's3://udacity-dend/song_data'\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE off\n",
    "    REGION 'us-west-2'\n",
    "    JSON 'auto';\n",
    "    \"\"\").format(config.get('IAM_ROLE', 'arn'))\n",
    "</codeblock>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Schema </h3>\n",
    "The datset schema is a star schema optimzied for queries on song play analysis using the song and log datasets. The star schema consists of a Fact Table, songplay_table, and 4 Dimension Tables: user_table, song_table, artist_table, and time_table. \n",
    "\n",
    "<ul>\n",
    "    <li> songplay_table - records in log data associated with song plays i.e. records with page NextSong</li>\n",
    "    <li> user_table - app users</li>\n",
    "    <li> song_table - songs in music database</li>\n",
    "    <li> artist_table - artists in music database</li>\n",
    "    <li> time_table - timestamps of records in songplays broken down into specific units</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Licensing, Authors, Acknowledgments</h3>\n",
    "\n",
    "Credit should be given to the Million Songs Dataset and Udacity for the data. Licensing for the data can be found at the Million Songs Dataset website <a href=\"https://labrosa.ee.columbia.edu/millionsong/\" rel=\"nofollow\">here</a> </li> and \n",
    "Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. \n",
    "The Million Song Dataset. In Proceedings of the 12th International Society\n",
    "for Music Information Retrieval Conference (ISMIR 2011), 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
